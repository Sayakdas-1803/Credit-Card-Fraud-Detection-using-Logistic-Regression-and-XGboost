# Credit-Card-Fraud-Detection-using-Logistic-Regression-and-XGboost
ğŸ“Œ Overview
This project implements a credit card fraud detection system using supervised machine learning techniques in R. The dataset is highly imbalanced, with fraudulent transactions forming a very small fraction of total observations. To address this, cost-sensitive learning and robust evaluation metrics are employed instead of naive accuracy-based approaches.

ğŸ“Š Dataset
- Source: Public credit card transaction dataset
- Records: European cardholdersâ€™ transactions
- Features:
  - `Time`: Time elapsed since first transaction
  - `Amount`: Transaction amount
  - `V1â€“V28`: PCA-transformed anonymized features
  - `Class`: Target variable (0 = Non-Fraud, 1 = Fraud)

ğŸ” Exploratory Data Analysis
- Severe class imbalance identified
- Density plots used to compare transaction timing
- Histograms used to analyze transaction amounts
- Log-scaled plots for better visualization of skewed distributions

âš™ï¸ Data Preprocessing
- Target variable converted to factor
- Time and Amount standardized
- Stratified trainâ€“test split (70â€“30)

ğŸ§  Models Implemented

1ï¸âƒ£ Logistic Regression (Class-Weighted)
- Baseline interpretable model
- Class imbalance handled using higher weights for fraud class
- Lower probability threshold used to improve fraud recall

2ï¸âƒ£ XGBoost (Imbalance-Aware)
- Tree-based ensemble model
- Handles non-linear relationships and feature interactions
- Class imbalance handled using `scale_pos_weight`
- Optimized for ROCâ€“AUC

ğŸ“ˆ Model Evaluation
- Confusion Matrix
- ROC Curve
- AUC Score

Accuracy is not emphasized due to imbalance; ROCâ€“AUC and recall are prioritized.

ğŸ“‰ Results
- Logistic Regression provides interpretability and stable baseline performance
- XGBoost achieves superior ROCâ€“AUC and better fraud detection capability
- Combined ROC curves clearly show performance differences

ğŸ›  Libraries Used
- `dplyr`
- `ggplot2`
- `caret`
- `pROC`
- `xgboost`
- `rpart`, `rpart.plot`

ğŸ§¾ Conclusion
Cost-sensitive learning significantly improves fraud detection in imbalanced datasets. While Logistic Regression offers interpretability, XGBoost captures complex fraud patterns more effectively, making it better suited for real-world deployment.
